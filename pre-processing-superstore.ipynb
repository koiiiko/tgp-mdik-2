{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c5d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ac9f7",
   "metadata": {},
   "source": [
    "Superstore pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816875fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"DeltaLake\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .config(\"spark.jars.packages\", \n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,io.delta:delta-spark_2.13:4.0.0\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLake with Hive Integration\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,io.delta:delta-spark_2.13:4.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///D:/ribkaadevina/college/8.S2/1. SEM 1/MDIK/tgp-mdik-2/hive/\") \\\n",
    "    .config(\"javax.jdo.option.ConnectionURL\", \"jdbc:derby:;databaseName=metastore_db;create=true\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"1200\") \\\n",
    "    .config(\"spark.network.timeout\", \"1200s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "df_superstore = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/delta_superstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cd7216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10851\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|message                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{'Row ID': '1', 'Order ID': 'CA-2016-152156', 'Order Date': '11/8/2016', 'Ship Date': '11/11/2016', 'Ship Mode': 'Second Class', 'Customer ID': 'CG-12520', 'Customer Name': 'Claire Gute', 'Segment': 'Consumer', 'Country': 'United States', 'City': 'Henderson', 'State': 'Kentucky', 'Postal Code': '42420', 'Region': 'South', 'Product ID': 'FUR-BO-10001798', 'Category': 'Furniture', 'Sub-Category': 'Bookcases', 'Product Name': 'Bush Somerset Collection Bookcase', 'Sales': '261.96', 'Quantity': '2', 'Discount': '0', 'Profit': '41.9136'}                                |\n",
      "|{'Row ID': '2', 'Order ID': 'CA-2016-152156', 'Order Date': '11/8/2016', 'Ship Date': '11/11/2016', 'Ship Mode': 'Second Class', 'Customer ID': 'CG-12520', 'Customer Name': 'Claire Gute', 'Segment': 'Consumer', 'Country': 'United States', 'City': 'Henderson', 'State': 'Kentucky', 'Postal Code': '42420', 'Region': 'South', 'Product ID': 'FUR-CH-10000454', 'Category': 'Furniture', 'Sub-Category': 'Chairs', 'Product Name': 'Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back', 'Sales': '731.94', 'Quantity': '3', 'Discount': '0', 'Profit': '219.582'}         |\n",
      "|{'Row ID': '3', 'Order ID': 'CA-2016-138688', 'Order Date': '6/12/2016', 'Ship Date': '6/16/2016', 'Ship Mode': 'Second Class', 'Customer ID': 'DV-13045', 'Customer Name': 'Darrin Van Huff', 'Segment': 'Corporate', 'Country': 'United States', 'City': 'Los Angeles', 'State': 'California', 'Postal Code': '90036', 'Region': 'West', 'Product ID': 'OFF-LA-10000240', 'Category': 'Office Supplies', 'Sub-Category': 'Labels', 'Product Name': 'Self-Adhesive Address Labels for Typewriters by Universal', 'Sales': '14.62', 'Quantity': '2', 'Discount': '0', 'Profit': '6.8714'}|\n",
      "|{'Row ID': '4', 'Order ID': 'US-2015-108966', 'Order Date': '10/11/2015', 'Ship Date': '10/18/2015', 'Ship Mode': 'Standard Class', 'Customer ID': 'SO-20335', 'Customer Name': \"Sean O'Donnell\", 'Segment': 'Consumer', 'Country': 'United States', 'City': 'Fort Lauderdale', 'State': 'Florida', 'Postal Code': '33311', 'Region': 'South', 'Product ID': 'FUR-TA-10000577', 'Category': 'Furniture', 'Sub-Category': 'Tables', 'Product Name': 'Bretford CR4500 Series Slim Rectangular Table', 'Sales': '957.5775', 'Quantity': '5', 'Discount': '0.45', 'Profit': '-383.031'}      |\n",
      "|{'Row ID': '5', 'Order ID': 'US-2015-108966', 'Order Date': '10/11/2015', 'Ship Date': '10/18/2015', 'Ship Mode': 'Standard Class', 'Customer ID': 'SO-20335', 'Customer Name': \"Sean O'Donnell\", 'Segment': 'Consumer', 'Country': 'United States', 'City': 'Fort Lauderdale', 'State': 'Florida', 'Postal Code': '33311', 'Region': 'South', 'Product ID': 'OFF-ST-10000760', 'Category': 'Office Supplies', 'Sub-Category': 'Storage', 'Product Name': \"Eldon Fold 'N Roll Cart System\", 'Sales': '22.368', 'Quantity': '2', 'Discount': '0.2', 'Profit': '2.5164'}                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(df_superstore.count())\n",
    "df_superstore.select(\"message\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aadd44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def parse_superstore(df):\n",
    "    superstore_schema = StructType([\n",
    "        StructField(\"Row ID\", StringType(), True),\n",
    "        StructField(\"Order ID\", StringType(), True),\n",
    "        StructField(\"Order Date\", StringType(), True),\n",
    "        StructField(\"Ship Date\", StringType(), True),\n",
    "        StructField(\"Ship Mode\", StringType(), True),\n",
    "        StructField(\"Customer ID\", StringType(), True),\n",
    "        StructField(\"Customer Name\", StringType(), True),\n",
    "        StructField(\"Segment\", StringType(), True),\n",
    "        StructField(\"Country\", StringType(), True),\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"State\", StringType(), True),\n",
    "        StructField(\"Postal Code\", StringType(), True),\n",
    "        StructField(\"Region\", StringType(), True),\n",
    "        StructField(\"Product ID\", StringType(), True),\n",
    "        StructField(\"Category\", StringType(), True),\n",
    "        StructField(\"Sub-Category\", StringType(), True),\n",
    "        StructField(\"Product Name\", StringType(), True),\n",
    "        StructField(\"Sales\", StringType(), True),\n",
    "        StructField(\"Quantity\", StringType(), True),\n",
    "        StructField(\"Discount\", StringType(), True),\n",
    "        StructField(\"Profit\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    parsed_superstore = df.select(\n",
    "        col(\"timestamp_kafka\"),\n",
    "        from_json(col(\"message\"), superstore_schema).alias(\"parsed_data\")\n",
    "    ).select(\n",
    "        col(\"timestamp_kafka\"),\n",
    "        col(\"parsed_data.Row ID\").cast(IntegerType()).alias(\"row_id\"),\n",
    "        col(\"parsed_data.Order ID\").alias(\"order_id\"),\n",
    "        to_date(col(\"parsed_data.Order Date\"), \"M/d/yyyy\").alias(\"order_date\"),\n",
    "        to_date(col(\"parsed_data.Ship Date\"), \"M/d/yyyy\").alias(\"ship_date\"),\n",
    "        col(\"parsed_data.Ship Mode\").alias(\"ship_mode\"),\n",
    "        col(\"parsed_data.Customer ID\").alias(\"customer_id\"),\n",
    "        col(\"parsed_data.Customer Name\").alias(\"customer_name\"),\n",
    "        col(\"parsed_data.Segment\").alias(\"segment\"),\n",
    "        col(\"parsed_data.Country\").alias(\"country\"),\n",
    "        col(\"parsed_data.City\").alias(\"city\"),\n",
    "        col(\"parsed_data.State\").alias(\"state\"),\n",
    "        col(\"parsed_data.Postal Code\").cast(IntegerType()).alias(\"postal_code\"),\n",
    "        col(\"parsed_data.Region\").alias(\"region\"),\n",
    "        col(\"parsed_data.Product ID\").alias(\"product_id\"),\n",
    "        col(\"parsed_data.Category\").alias(\"category\"),\n",
    "        col(\"parsed_data.Sub-Category\").alias(\"sub_category\"),\n",
    "        col(\"parsed_data.Product Name\").alias(\"product_name\"),\n",
    "        col(\"parsed_data.Sales\").cast(DecimalType(10,2)).alias(\"sales\"),\n",
    "        col(\"parsed_data.Quantity\").cast(IntegerType()).alias(\"quantity\"),\n",
    "        col(\"parsed_data.Discount\").cast(DecimalType(5,4)).alias(\"discount\"),\n",
    "        col(\"parsed_data.Profit\").cast(DecimalType(10,2)).alias(\"profit\")\n",
    "    )\n",
    "    \n",
    "    return parsed_superstore\n",
    "df_parsed_superstore = parse_superstore(df_superstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd647dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "['timestamp_kafka', 'row_id', 'order_id', 'order_date', 'ship_date', 'ship_mode', 'customer_id', 'customer_name', 'segment', 'country', 'city', 'state', 'postal_code', 'region', 'product_id', 'category', 'sub_category', 'product_name', 'sales', 'quantity', 'discount', 'profit'] \n",
      "\n",
      "+--------------------+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------+--------+--------+-------+\n",
      "|     timestamp_kafka|row_id|      order_id|order_date| ship_date|     ship_mode|customer_id|  customer_name|  segment|      country|           city|     state|postal_code|region|     product_id|       category|sub_category|        product_name| sales|quantity|discount| profit|\n",
      "+--------------------+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------+--------+--------+-------+\n",
      "|2025-06-25 23:05:...|     1|CA-2016-152156|2016-11-08|2016-11-11|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|261.96|       2|  0.0000|  41.91|\n",
      "|2025-06-25 23:05:...|     2|CA-2016-152156|2016-11-08|2016-11-11|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|731.94|       3|  0.0000| 219.58|\n",
      "|2025-06-25 23:05:...|     3|CA-2016-138688|2016-06-12|2016-06-16|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|      90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...| 14.62|       2|  0.0000|   6.87|\n",
      "|2025-06-25 23:05:...|     4|US-2015-108966|2015-10-11|2015-10-18|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.58|       5|  0.4500|-383.03|\n",
      "|2025-06-25 23:05:...|     5|US-2015-108966|2015-10-11|2015-10-18|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...| 22.37|       2|  0.2000|   2.52|\n",
      "+--------------------+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------+--------+--------+-------+\n",
      "only showing top 5 rows\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(len(df_parsed_superstore.columns))\n",
    "print(f\"{(df_parsed_superstore.columns)} \\n\")\n",
    "print(df_parsed_superstore.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1575f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------+----------+----------+--------------+-----------+----------------+--------+-------------+-----------+----------+-----------+------+---------------+---------------+------------+--------------------+------+--------+--------+------+\n",
      "|     timestamp_kafka|row_id|      order_id|order_date| ship_date|     ship_mode|customer_id|   customer_name| segment|      country|       city|     state|postal_code|region|     product_id|       category|sub_category|        product_name| sales|quantity|discount|profit|\n",
      "+--------------------+------+--------------+----------+----------+--------------+-----------+----------------+--------+-------------+-----------+----------+-----------+------+---------------+---------------+------------+--------------------+------+--------+--------+------+\n",
      "|2025-06-25 23:14:...|  9994|CA-2017-119914|2017-05-04|2017-05-09|  Second Class|   CC-12220|    Chris Cortes|Consumer|United States|Westminster|California|      92683|  West|OFF-AP-10002684|Office Supplies|  Appliances|Acco 7-Outlet Mas...|243.16|       2|  0.0000| 72.95|\n",
      "|2025-06-25 23:14:...|  9993|CA-2017-121258|2017-02-26|2017-03-03|Standard Class|   DB-13060|     Dave Brooks|Consumer|United States| Costa Mesa|California|      92627|  West|OFF-PA-10004041|Office Supplies|       Paper|It's Hot Message ...| 29.60|       4|  0.0000| 13.32|\n",
      "|2025-06-25 23:14:...|  9992|CA-2017-121258|2017-02-26|2017-03-03|Standard Class|   DB-13060|     Dave Brooks|Consumer|United States| Costa Mesa|California|      92627|  West|TEC-PH-10003645|     Technology|      Phones|Aastra 57i VoIP p...|258.58|       2|  0.2000| 19.39|\n",
      "|2025-06-25 23:14:...|  9991|CA-2017-121258|2017-02-26|2017-03-03|Standard Class|   DB-13060|     Dave Brooks|Consumer|United States| Costa Mesa|California|      92627|  West|FUR-FU-10000747|      Furniture| Furnishings|Tenex B1-RE Serie...| 91.96|       2|  0.0000| 15.63|\n",
      "|2025-06-25 23:14:...|  9990|CA-2014-110422|2014-01-21|2014-01-23|  Second Class|   TB-21400|Tom Boeckenhauer|Consumer|United States|      Miami|   Florida|      33180| South|FUR-FU-10001889|      Furniture| Furnishings|Ultra Door Pull H...| 25.25|       3|  0.2000|  4.10|\n",
      "+--------------------+------+--------------+----------+----------+--------------+-----------+----------------+--------+-------------+-----------+----------+-----------+------+---------------+---------------+------------+--------------------+------+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parsed_superstore.orderBy(col(\"row_id\").desc()).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6d685",
   "metadata": {},
   "source": [
    "Check Null Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d26e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------+--------+----------+---------+---------+-----------+-------------+-------+-------+----+-----+-----------+------+----------+--------+------------+------------+-----+--------+--------+------+\n",
      "|timestamp_kafka        |row_id|order_id|order_date|ship_date|ship_mode|customer_id|customer_name|segment|country|city|state|postal_code|region|product_id|category|sub_category|product_name|sales|quantity|discount|profit|\n",
      "+-----------------------+------+--------+----------+---------+---------+-----------+-------------+-------+-------+----+-----+-----------+------+----------+--------+------------+------------+-----+--------+--------+------+\n",
      "|2025-06-25 23:05:50.342|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "|2025-06-25 23:05:50.342|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "|2025-06-25 23:05:50.342|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "|2025-06-25 23:05:50.342|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "|2025-06-25 23:05:50.385|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "|2025-06-25 23:05:50.386|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "|2025-06-25 23:05:50.387|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "|2025-06-25 23:05:50.388|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "|2025-06-25 23:05:50.391|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "|2025-06-25 23:05:50.392|NULL  |NULL    |NULL      |NULL     |NULL     |NULL       |NULL         |NULL   |NULL   |NULL|NULL |NULL       |NULL  |NULL      |NULL    |NULL        |NULL        |NULL |NULL    |NULL    |NULL  |\n",
      "+-----------------------+------+--------+----------+---------+---------+-----------+-------------+-------+-------+----+-----+-----------+------+----------+--------+------------+------------+-----+--------+--------+------+\n",
      "only showing top 10 rows\n",
      "Total null: 307\n"
     ]
    }
   ],
   "source": [
    "all_null_rows = df_parsed_superstore.filter(\n",
    "    col(\"row_id\").isNull() & \n",
    "    col(\"order_id\").isNull() & \n",
    "    col(\"customer_name\").isNull()\n",
    ")\n",
    "all_null_rows.show(10, truncate=False)\n",
    "print(f\"Total null: {all_null_rows.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b8ac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10544\n"
     ]
    }
   ],
   "source": [
    "df_parsed_superstore=df_parsed_superstore.dropna()\n",
    "print(df_parsed_superstore.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f406ac",
   "metadata": {},
   "source": [
    "Check Data Anomali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07722eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "negative_sales = df_parsed_superstore.filter(col(\"sales\") < 0).count()\n",
    "negative_quantity = df_parsed_superstore.filter(col(\"quantity\") < 0).count()\n",
    "negative_discount = df_parsed_superstore.filter(col(\"discount\") < 0).count()\n",
    "print (negative_sales, negative_quantity , negative_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b99791",
   "metadata": {},
   "source": [
    "Check Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b259b09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 10544\n",
      "Distinct data: 10544\n",
      "Duplicate data: 0\n"
     ]
    }
   ],
   "source": [
    "total_data = df_parsed_superstore.count()\n",
    "distinct_data = df_parsed_superstore.distinct().count()\n",
    "duplicate_data = total_data - distinct_data\n",
    "\n",
    "print(f\"Total data: {total_data}\")\n",
    "print(f\"Distinct data: {distinct_data}\")\n",
    "print(f\"Duplicate data: {duplicate_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e76e2",
   "metadata": {},
   "source": [
    "Create DW Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7922175a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS db_tgp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e96a945b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE db_tgp2\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_date\")\n",
    "spark.sql(\"CREATE TABLE dim_date (date_id INT, date INT, month INT, year INT) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_ship\")\n",
    "spark.sql(\"CREATE TABLE dim_ship (ship_id INT, ship_mode STRING) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_city\")\n",
    "spark.sql(\"CREATE TABLE dim_city (city_id INT, city_name STRING) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_customer\")\n",
    "spark.sql(\"CREATE TABLE dim_customer (customer_id INT, customer_name STRING, zipcode STRING) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_state\")\n",
    "spark.sql(\"CREATE TABLE dim_state (state_id INT, state_name STRING) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_segment\")\n",
    "spark.sql(\"CREATE TABLE dim_segment (segment_id INT, segment STRING) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_region\")\n",
    "spark.sql(\"CREATE TABLE dim_region (region_id INT, region STRING) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_product\")\n",
    "spark.sql(\"CREATE TABLE dim_product (product_id INT, product_name STRING) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_product_category\")\n",
    "spark.sql(\"CREATE TABLE dim_product_category (category_id INT, product_category STRING) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_product_subcategory\")\n",
    "spark.sql(\"CREATE TABLE dim_product_subcategory (subcategory_id INT, product_subcategory STRING) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS order_fact\")\n",
    "spark.sql(\"CREATE TABLE order_fact (order_id INT, sales DECIMAL(10,2), quantity INT, profits DECIMAL(10,2), discount DECIMAL(5,2)) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_product_hierarchy\")\n",
    "spark.sql(\"CREATE TABLE dim_product_hierarchy (product_id INT, category_id INT, subcategory_id INT) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_order\")\n",
    "spark.sql(\"CREATE TABLE dim_order (order_id INT, product_id INT, customer_id INT, ship_mode_id INT, order_date_id INT, shipment_date_id INT) USING hive\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_customer_location\")\n",
    "spark.sql(\"CREATE TABLE dim_customer_location (customer_id INT, city_id INT, state_id INT, segment_id INT, region_id INT) USING hive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100488d6",
   "metadata": {},
   "source": [
    "Send to Hive (Data Warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa08fc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "spark.sql(\"USE db_tgp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00082363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+\n",
      "|date_id|date|month|year|\n",
      "+-------+----+-----+----+\n",
      "|      1|   3|    1|2014|\n",
      "|      2|   4|    1|2014|\n",
      "|      3|   5|    1|2014|\n",
      "|      4|   6|    1|2014|\n",
      "|      5|   7|    1|2014|\n",
      "|      6|   8|    1|2014|\n",
      "|      7|   9|    1|2014|\n",
      "|      8|  10|    1|2014|\n",
      "|      9|  11|    1|2014|\n",
      "|     10|  12|    1|2014|\n",
      "|     11|  13|    1|2014|\n",
      "|     12|  14|    1|2014|\n",
      "|     13|  15|    1|2014|\n",
      "|     14|  16|    1|2014|\n",
      "|     15|  17|    1|2014|\n",
      "|     16|  18|    1|2014|\n",
      "|     17|  19|    1|2014|\n",
      "|     18|  20|    1|2014|\n",
      "|     19|  21|    1|2014|\n",
      "|     20|  23|    1|2014|\n",
      "+-------+----+-----+----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# dim_date\n",
    "dates = df_parsed_superstore.select(\"order_date\").union(df_parsed_superstore.select(col(\"ship_date\").alias(\"order_date\"))).distinct()\n",
    "windowSpec = Window.orderBy(\"order_date\")\n",
    "dim_date = dates.withColumn(\"date_id\", row_number().over(windowSpec)).select(\n",
    "    col(\"date_id\"),\n",
    "    dayofmonth(col(\"order_date\")).alias(\"date\"),\n",
    "    month(\"order_date\").alias(\"month\"),\n",
    "    year(\"order_date\").alias(\"year\")\n",
    ")\n",
    "dim_date.write.mode(\"overwrite\").saveAsTable(\"dim_date\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM dim_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bd56c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|ship_id|     ship_mode|\n",
      "+-------+--------------+\n",
      "|      1|   First Class|\n",
      "|      2|      Same Day|\n",
      "|      3|  Second Class|\n",
      "|      4|Standard Class|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dim_ship\n",
    "dim_ship = df_parsed_superstore.select(\"ship_mode\").distinct()\n",
    "windowSpec = Window.orderBy(\"ship_mode\")\n",
    "dim_ship = dim_ship.withColumn(\"ship_id\", row_number().over(windowSpec)).select(\"ship_id\", \"ship_mode\")\n",
    "dim_ship.write.mode(\"overwrite\").saveAsTable(\"dim_ship\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM dim_ship\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc2bea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|city_id|        city_name|\n",
      "+-------+-----------------+\n",
      "|      1|         Aberdeen|\n",
      "|      2|          Abilene|\n",
      "|      3|            Akron|\n",
      "|      4|      Albuquerque|\n",
      "|      5|       Alexandria|\n",
      "|      6|            Allen|\n",
      "|      7|        Allentown|\n",
      "|      8|          Altoona|\n",
      "|      9|         Amarillo|\n",
      "|     10|          Anaheim|\n",
      "|     11|          Andover|\n",
      "|     12|        Ann Arbor|\n",
      "|     13|          Antioch|\n",
      "|     14|           Apopka|\n",
      "|     15|     Apple Valley|\n",
      "|     16|         Appleton|\n",
      "|     17|        Arlington|\n",
      "|     18|Arlington Heights|\n",
      "|     19|           Arvada|\n",
      "|     20|        Asheville|\n",
      "+-------+-----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# dim_city\n",
    "dim_city = df_parsed_superstore.select(\"city\").distinct()\n",
    "windowSpec = Window.orderBy(\"city\")\n",
    "dim_city = dim_city.withColumn(\"city_id\", row_number().over(windowSpec)).select(\n",
    "    col(\"city_id\"), col(\"city\").alias(\"city_name\")\n",
    ")\n",
    "dim_city.write.mode(\"overwrite\").saveAsTable(\"dim_city\")\n",
    "spark.sql(\"SELECT * FROM dim_city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35be4702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------+\n",
      "|customer_id|  customer_name|zipcode|\n",
      "+-----------+---------------+-------+\n",
      "|          1|  Aaron Bergman|  76017|\n",
      "|          2|  Aaron Bergman|  98103|\n",
      "|          3|  Aaron Bergman|  73120|\n",
      "|          4|  Aaron Hawkins|  90004|\n",
      "|          5|  Aaron Hawkins|  94122|\n",
      "|          6|  Aaron Hawkins|  12180|\n",
      "|          7|  Aaron Hawkins|  39503|\n",
      "|          8|  Aaron Hawkins|  10035|\n",
      "|          9|  Aaron Hawkins|  19134|\n",
      "|         10|  Aaron Hawkins|  94109|\n",
      "|         11| Aaron Smayling|  22204|\n",
      "|         12| Aaron Smayling|  94110|\n",
      "|         13| Aaron Smayling|  91104|\n",
      "|         14| Aaron Smayling|  78745|\n",
      "|         15| Aaron Smayling|  10035|\n",
      "|         16| Aaron Smayling|  97756|\n",
      "|         17| Aaron Smayling|  28540|\n",
      "|         18|Adam Bellavance|  98198|\n",
      "|         19|Adam Bellavance|  22980|\n",
      "|         20|Adam Bellavance|  98105|\n",
      "+-----------+---------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# dim_customer\n",
    "dim_customer = df_parsed_superstore.select(\"customer_id\", \"customer_name\", \"postal_code\").distinct()\n",
    "windowSpec = Window.orderBy(\"customer_name\")\n",
    "dim_customer = dim_customer.withColumn(\"customer_id\", row_number().over(windowSpec)).select(\n",
    "    col(\"customer_id\"), col(\"customer_name\"), col(\"postal_code\").alias(\"zipcode\")\n",
    ")\n",
    "dim_customer.write.mode(\"overwrite\").saveAsTable(\"dim_customer\")\n",
    "spark.sql(\"SELECT * FROM dim_customer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b664e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|state_id|          state_name|\n",
      "+--------+--------------------+\n",
      "|       1|             Alabama|\n",
      "|       2|             Arizona|\n",
      "|       3|            Arkansas|\n",
      "|       4|          California|\n",
      "|       5|            Colorado|\n",
      "|       6|         Connecticut|\n",
      "|       7|            Delaware|\n",
      "|       8|District of Columbia|\n",
      "|       9|             Florida|\n",
      "|      10|             Georgia|\n",
      "|      11|               Idaho|\n",
      "|      12|            Illinois|\n",
      "|      13|             Indiana|\n",
      "|      14|                Iowa|\n",
      "|      15|              Kansas|\n",
      "|      16|            Kentucky|\n",
      "|      17|           Louisiana|\n",
      "|      18|               Maine|\n",
      "|      19|            Maryland|\n",
      "|      20|       Massachusetts|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# dim_state\n",
    "dim_state = df_parsed_superstore.select(\"state\").distinct()\n",
    "windowSpec = Window.orderBy(\"state\")\n",
    "dim_state = dim_state.withColumn(\"state_id\", row_number().over(windowSpec)).select(\n",
    "    col(\"state_id\"), col(\"state\").alias(\"state_name\")\n",
    ")\n",
    "dim_state.write.mode(\"overwrite\").saveAsTable(\"dim_state\")\n",
    "spark.sql(\"SELECT * FROM dim_state\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9259ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|segment_id|    segment|\n",
      "+----------+-----------+\n",
      "|         1|   Consumer|\n",
      "|         2|  Corporate|\n",
      "|         3|Home Office|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dim_segment\n",
    "dim_segment = df_parsed_superstore.select(\"segment\").distinct()\n",
    "windowSpec = Window.orderBy(\"segment\")\n",
    "dim_segment = dim_segment.withColumn(\"segment_id\", row_number().over(windowSpec)).select(\"segment_id\", \"segment\")\n",
    "dim_segment.write.mode(\"overwrite\").saveAsTable(\"dim_segment\")\n",
    "spark.sql(\"SELECT * FROM dim_segment\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b34d55a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|region_id| region|\n",
      "+---------+-------+\n",
      "|        1|Central|\n",
      "|        2|   East|\n",
      "|        3|  South|\n",
      "|        4|   West|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dim_region\n",
    "dim_region = df_parsed_superstore.select(\"region\").distinct()\n",
    "windowSpec = Window.orderBy(\"region\")\n",
    "dim_region = dim_region.withColumn(\"region_id\", row_number().over(windowSpec)).select(\"region_id\", \"region\")\n",
    "dim_region.write.mode(\"overwrite\").saveAsTable(\"dim_region\")\n",
    "spark.sql(\"SELECT * FROM dim_region\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c04016ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|product_id|        product_name|\n",
      "+----------+--------------------+\n",
      "|         1|\"While you Were O...|\n",
      "|         2|#10 Gummed Flap W...|\n",
      "|         3|#10 Self-Seal Whi...|\n",
      "|         4|#10 White Busines...|\n",
      "|         5|#10- 4 1/8\" x 9 1...|\n",
      "|         6|#10- 4 1/8\" x 9 1...|\n",
      "|         7|#10- 4 1/8\" x 9 1...|\n",
      "|         8|#10-4 1/8\" x 9 1/...|\n",
      "|         9|#6 3/4 Gummed Fla...|\n",
      "|        10|1.7 Cubic Foot Co...|\n",
      "|        11|1/4 Fold Party De...|\n",
      "|        12|12 Colored Short ...|\n",
      "|        13|12-1/2 Diameter R...|\n",
      "|        14|14-7/8 x 11 Blue ...|\n",
      "|        15|2300 Heavy-Duty T...|\n",
      "|        16|24 Capacity Maxi ...|\n",
      "|        17|24-Hour Round Wal...|\n",
      "|        18|  3-ring staple pack|\n",
      "|        19|3.6 Cubic Foot Co...|\n",
      "|        20|36X48 HARDFLOOR C...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# dim_product\n",
    "dim_product = df_parsed_superstore.select(\"product_id\", \"product_name\").distinct()\n",
    "windowSpec = Window.orderBy(\"product_name\")\n",
    "dim_product = dim_product.withColumn(\"product_id\", row_number().over(windowSpec)).select(\"product_id\", \"product_name\")\n",
    "dim_product.write.mode(\"overwrite\").saveAsTable(\"dim_product\")\n",
    "spark.sql(\"SELECT * FROM dim_product\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dc92782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|category_id|product_category|\n",
      "+-----------+----------------+\n",
      "|          1|       Furniture|\n",
      "|          2| Office Supplies|\n",
      "|          3|      Technology|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dim_product_category\n",
    "dim_product_category = df_parsed_superstore.select(\"category\").distinct()\n",
    "windowSpec = Window.orderBy(\"category\")\n",
    "dim_product_category = dim_product_category.withColumn(\"category_id\", row_number().over(windowSpec)).select(\n",
    "    col(\"category_id\"), col(\"category\").alias(\"product_category\")\n",
    ")\n",
    "dim_product_category.write.mode(\"overwrite\").saveAsTable(\"dim_product_category\")\n",
    "spark.sql(\"SELECT * FROM dim_product_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e947797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|subcategory_id|product_subcategory|\n",
      "+--------------+-------------------+\n",
      "|             1|        Accessories|\n",
      "|             2|         Appliances|\n",
      "|             3|                Art|\n",
      "|             4|            Binders|\n",
      "|             5|          Bookcases|\n",
      "|             6|             Chairs|\n",
      "|             7|            Copiers|\n",
      "|             8|          Envelopes|\n",
      "|             9|          Fasteners|\n",
      "|            10|        Furnishings|\n",
      "|            11|             Labels|\n",
      "|            12|           Machines|\n",
      "|            13|              Paper|\n",
      "|            14|             Phones|\n",
      "|            15|            Storage|\n",
      "|            16|           Supplies|\n",
      "|            17|             Tables|\n",
      "+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dim_product_subcategory\n",
    "dim_product_subcategory = df_parsed_superstore.select(\"sub_category\").distinct()\n",
    "windowSpec = Window.orderBy(\"sub_category\")\n",
    "dim_product_subcategory = dim_product_subcategory.withColumn(\"subcategory_id\", row_number().over(windowSpec)).select(\n",
    "    col(\"subcategory_id\"), col(\"sub_category\").alias(\"product_subcategory\")\n",
    ")\n",
    "dim_product_subcategory.write.mode(\"overwrite\").saveAsTable(\"dim_product_subcategory\")\n",
    "spark.sql(\"SELECT * FROM dim_product_subcategory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb540e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------+\n",
      "|product_id|category_id|subcategory_id|\n",
      "+----------+-----------+--------------+\n",
      "|      1014|          2|            16|\n",
      "|      1430|          2|            13|\n",
      "|       590|          1|            10|\n",
      "|       187|          2|            11|\n",
      "|      1296|          2|             8|\n",
      "|       184|          2|            11|\n",
      "|      1303|          2|            13|\n",
      "|      1219|          2|             3|\n",
      "|      1241|          3|             1|\n",
      "|      1256|          2|            15|\n",
      "|       881|          1|            10|\n",
      "|       147|          3|            14|\n",
      "|      1397|          2|            11|\n",
      "|       989|          3|             1|\n",
      "|      1451|          2|            15|\n",
      "|         6|          2|             8|\n",
      "|         5|          2|             8|\n",
      "|        98|          2|            16|\n",
      "|       739|          1|             6|\n",
      "|       108|          2|            13|\n",
      "+----------+-----------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# dim_product_hierarchy\n",
    "product_hier = df_parsed_superstore.select(\"product_name\", \"category\", \"sub_category\").distinct()\n",
    "\n",
    "product_lookup = spark.sql(\"SELECT product_id, product_name FROM dim_product\")\n",
    "category_lookup = spark.sql(\"SELECT category_id, product_category FROM dim_product_category\")\n",
    "subcategory_lookup = spark.sql(\"SELECT subcategory_id, product_subcategory FROM dim_product_subcategory\")\n",
    "\n",
    "product_hierarchy = product_hier.join(\n",
    "    product_lookup, product_hier.product_name == product_lookup.product_name, \"inner\"\n",
    ").join(\n",
    "    category_lookup, product_hier.category == category_lookup.product_category, \"inner\"\n",
    ").join(\n",
    "    subcategory_lookup, product_hier.sub_category == subcategory_lookup.product_subcategory, \"inner\"\n",
    ").select(\n",
    "    product_lookup.product_id,\n",
    "    category_lookup.category_id,\n",
    "    subcategory_lookup.subcategory_id\n",
    ")\n",
    "\n",
    "product_hierarchy.write.mode(\"overwrite\").saveAsTable(\"dim_product_hierarchy\")\n",
    "spark.sql(\"SELECT * FROM dim_product_hierarchy\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd5a397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+----------+---------+\n",
      "|customer_id|city_id|state_id|segment_id|region_id|\n",
      "+-----------+-------+--------+----------+---------+\n",
      "|       4828|    428|      36|         1|        4|\n",
      "|       4827|    428|      36|         1|        4|\n",
      "|       4826|    428|      36|         1|        4|\n",
      "|       4825|    428|      36|         1|        4|\n",
      "|       4824|    428|      36|         1|        4|\n",
      "|       4823|    428|      36|         1|        4|\n",
      "|       4822|    428|      36|         1|        4|\n",
      "|       4821|    428|      36|         1|        4|\n",
      "|       4820|    428|      36|         1|        4|\n",
      "|       4819|    428|      36|         1|        4|\n",
      "|       4818|    428|      36|         1|        4|\n",
      "|       4817|    428|      36|         1|        4|\n",
      "|       4816|    428|      36|         1|        4|\n",
      "|       1193|    450|      46|         3|        4|\n",
      "|       1192|    450|      46|         3|        4|\n",
      "|       1191|    450|      46|         3|        4|\n",
      "|       1190|    450|      46|         3|        4|\n",
      "|       2172|     61|      32|         1|        3|\n",
      "|       2171|     61|      32|         1|        3|\n",
      "|       2170|     61|      32|         1|        3|\n",
      "+-----------+-------+--------+----------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# dim_customer_location\n",
    "cust_loc = df_parsed_superstore.select(\"customer_name\", \"city\", \"state\", \"segment\", \"region\").distinct()\n",
    "\n",
    "customer_lookup = spark.sql(\"SELECT customer_id, customer_name FROM dim_customer\")\n",
    "city_lookup = spark.sql(\"SELECT city_id, city_name FROM dim_city\")\n",
    "state_lookup = spark.sql(\"SELECT state_id, state_name FROM dim_state\")\n",
    "segment_lookup = spark.sql(\"SELECT segment_id, segment FROM dim_segment\")\n",
    "region_lookup = spark.sql(\"SELECT region_id, region FROM dim_region\")\n",
    "\n",
    "customer_location = cust_loc.join(\n",
    "    customer_lookup, cust_loc.customer_name == customer_lookup.customer_name, \"inner\"\n",
    ").join(\n",
    "    city_lookup, cust_loc.city == city_lookup.city_name, \"inner\"\n",
    ").join(\n",
    "    state_lookup, cust_loc.state == state_lookup.state_name, \"inner\"\n",
    ").join(\n",
    "    segment_lookup, cust_loc.segment == segment_lookup.segment, \"inner\"\n",
    ").join(\n",
    "    region_lookup, cust_loc.region == region_lookup.region, \"inner\"\n",
    ").select(\n",
    "    customer_lookup.customer_id,\n",
    "    city_lookup.city_id,\n",
    "    state_lookup.state_id,\n",
    "    segment_lookup.segment_id,\n",
    "    region_lookup.region_id\n",
    ")\n",
    "\n",
    "customer_location.write.mode(\"overwrite\").saveAsTable(\"dim_customer_location\")\n",
    "spark.sql(\"SELECT * FROM dim_customer_location\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04efe2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------+--------+-------+---------------------+\n",
      "| id|      order_id| sales|quantity|profits|total_discount_amount|\n",
      "+---+--------------+------+--------+-------+---------------------+\n",
      "|  1|CA-2014-100006|377.97|       3| 109.61|             0.000000|\n",
      "|  2|CA-2014-100090|699.19|       9| -19.09|           139.838000|\n",
      "|  3|CA-2014-100293| 91.06|       6|  31.87|            18.212000|\n",
      "|  4|CA-2014-100328|  3.93|       1|   1.33|             0.786000|\n",
      "|  5|CA-2014-100363| 21.38|       5|   7.72|             4.276000|\n",
      "|  6|CA-2014-100391| 14.62|       2|   6.73|             0.000000|\n",
      "|  7|CA-2014-100678|697.08|      11|  61.80|           171.122000|\n",
      "|  8|CA-2014-100706|129.44|       8|  17.72|             0.000000|\n",
      "|  9|CA-2014-100762|508.62|      11| 219.08|             0.000000|\n",
      "| 10|CA-2014-100860| 18.75|       5|   9.00|             0.000000|\n",
      "| 11|CA-2014-100867|321.55|       6|  20.10|            64.310000|\n",
      "| 12|CA-2014-100881|302.38|       3|  22.68|            60.476000|\n",
      "| 13|CA-2014-100895|605.47|       7| 176.92|             0.000000|\n",
      "| 14|CA-2014-100916|788.86|      10| 122.97|             0.000000|\n",
      "| 15|CA-2014-100972|166.44|       3|  79.89|             0.000000|\n",
      "| 16|CA-2014-101147|  2.39|       1|  -6.34|             1.912000|\n",
      "| 17|CA-2014-101175|100.70|       6|  -1.26|            20.140000|\n",
      "| 18|CA-2014-101266| 13.36|       2|   6.41|             0.000000|\n",
      "| 19|CA-2014-101364|296.71|      13| 100.14|            59.342000|\n",
      "| 20|CA-2014-101392|269.36|       7|  70.03|             0.000000|\n",
      "+---+--------------+------+--------+-------+---------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# order_fact\n",
    "order_fact = df_parsed_superstore.groupBy(\"order_id\").agg(\n",
    "    sum(\"sales\").alias(\"sales\"),\n",
    "    sum(\"quantity\").alias(\"quantity\"), \n",
    "    sum(\"profit\").alias(\"profits\"),\n",
    "    sum(col(\"sales\") * col(\"discount\")).alias(\"total_discount_amount\")\n",
    ").withColumn(\"id\", row_number().over(Window.orderBy(\"order_id\")))  # Surrogate key\n",
    "\n",
    "order_fact = order_fact.select(\"id\", \"order_id\", \"sales\", \"quantity\", \"profits\", \"total_discount_amount\")\n",
    "\n",
    "order_fact.write.mode(\"overwrite\").saveAsTable(\"order_fact\")\n",
    "spark.sql(\"SELECT * FROM order_fact\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6bfeccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+---------------+----------------+-----------------+--------------------+\n",
      "|dim_order_id|dim_product_id|dim_customer_id|dim_ship_mode_id|dim_order_date_id|dim_shipment_date_id|\n",
      "+------------+--------------+---------------+----------------+-----------------+--------------------+\n",
      "|           1|            46|           1338|               4|              237|                 243|\n",
      "|           1|            46|           1339|               4|              237|                 243|\n",
      "|           1|            46|           1337|               4|              237|                 243|\n",
      "|           1|            46|           1335|               4|              237|                 243|\n",
      "|           1|            46|           1336|               4|              237|                 243|\n",
      "|           1|            46|           1333|               4|              237|                 243|\n",
      "|           1|            46|           1332|               4|              237|                 243|\n",
      "|           1|            46|           1334|               4|              237|                 243|\n",
      "|           2|           826|           1464|               4|              178|                 182|\n",
      "|           2|           826|           1462|               4|              178|                 182|\n",
      "|           2|           826|           1460|               4|              178|                 182|\n",
      "|           2|           826|           1461|               4|              178|                 182|\n",
      "|           2|          1628|           1464|               4|              178|                 182|\n",
      "|           2|          1628|           1461|               4|              178|                 182|\n",
      "|           2|          1628|           1468|               4|              178|                 182|\n",
      "|           2|           826|           1467|               4|              178|                 182|\n",
      "|           2|          1628|           1462|               4|              178|                 182|\n",
      "|           2|          1628|           1463|               4|              178|                 182|\n",
      "|           2|           826|           1466|               4|              178|                 182|\n",
      "|           2|          1628|           1467|               4|              178|                 182|\n",
      "+------------+--------------+---------------+----------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, dayofmonth, month, year, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Data dasar dari source\n",
    "base_order = df_parsed_superstore.select(\n",
    "    \"order_id\", \"product_name\", \"customer_name\", \"ship_mode\", \"order_date\", \"ship_date\"\n",
    ")\n",
    "\n",
    "# Step 2: Ambil dimensi\n",
    "order_fact = spark.sql(\"SELECT id AS dim_order_id, order_id FROM order_fact\")\n",
    "product = spark.sql(\"SELECT product_id, product_name FROM dim_product\")\n",
    "customer = spark.sql(\"SELECT customer_id, customer_name FROM dim_customer\")\n",
    "ship = spark.sql(\"SELECT ship_id, ship_mode FROM dim_ship\")\n",
    "date = spark.sql(\"SELECT date_id, date, month, year FROM dim_date\")\n",
    "\n",
    "# Step 3: Ambil 1 customer_name per order_id\n",
    "window_order = Window.partitionBy(\"order_id\").orderBy(\"customer_name\")\n",
    "order_customer_df = df_parsed_superstore \\\n",
    "    .select(\"order_id\", \"customer_name\") \\\n",
    "    .withColumn(\"row_num\", row_number().over(window_order)) \\\n",
    "    .filter(\"row_num = 1\") \\\n",
    "    .drop(\"row_num\")\n",
    "\n",
    "order_customer = order_customer_df.join(customer, on=\"customer_name\", how=\"inner\")\n",
    "\n",
    "# Step 4: Join semua\n",
    "dim_order = base_order \\\n",
    "    .join(order_fact, on=\"order_id\", how=\"inner\") \\\n",
    "    .join(order_customer.select(\"order_id\", \"customer_id\"), on=\"order_id\", how=\"inner\") \\\n",
    "    .join(product, on=\"product_name\", how=\"inner\") \\\n",
    "    .join(ship, on=\"ship_mode\", how=\"inner\") \\\n",
    "    .join(\n",
    "        date.alias(\"od\"),\n",
    "        (dayofmonth(\"order_date\") == col(\"od.date\")) &\n",
    "        (month(\"order_date\") == col(\"od.month\")) &\n",
    "        (year(\"order_date\") == col(\"od.year\")),\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .join(\n",
    "        date.alias(\"sd\"),\n",
    "        (dayofmonth(\"ship_date\") == col(\"sd.date\")) &\n",
    "        (month(\"ship_date\") == col(\"sd.month\")) &\n",
    "        (year(\"ship_date\") == col(\"sd.year\")),\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"dim_order_id\"),\n",
    "        col(\"product_id\").alias(\"dim_product_id\"),\n",
    "        col(\"customer_id\").alias(\"dim_customer_id\"),\n",
    "        col(\"ship_id\").alias(\"dim_ship_mode_id\"),\n",
    "        col(\"od.date_id\").alias(\"dim_order_date_id\"),\n",
    "        col(\"sd.date_id\").alias(\"dim_shipment_date_id\")\n",
    "    ).dropDuplicates()\n",
    "\n",
    "# Step 5: Simpan ke Hive\n",
    "dim_order.write.mode(\"overwrite\").saveAsTable(\"dim_order\")\n",
    "# Cek hasilnya\n",
    "spark.sql(\"SELECT * FROM dim_order ORDER BY dim_order_id\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
